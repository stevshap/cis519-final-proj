# -*- coding: utf-8 -*-
"""LSTM2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s41aoBqX9lxz_mFh_Ab941PfieIiRx-_
"""

## IMPORTS 
import numpy as np 
import pandas as pd
import scipy
from scipy import signal
from scipy.io import loadmat
from sklearn import preprocessing 
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import shuffle 
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from keras.utils import to_categorical

# Load in full data set from MATLAB
data = loadmat('cis519project_data.mat')

# Store data into arrays 
full_dg_p1 = data['full_dg_p1']; full_dg_p2 = data['full_dg_p2']; full_dg_p3 = data['full_dg_p3']
full_ecog_p1 = data['full_ecog_p1']; full_ecog_p2 = data['full_ecog_p2']; full_ecog_p3 = data['full_ecog_p3']

"""Prepare Labels """

# make individual finger flexion arrays 
dg_finger1 = []; dg_finger2 = []; dg_finger3 = []; dg_finger4 = []; dg_finger5 = [] 
for array in full_dg_p1: 
    dg_finger1.append(array[0])
    dg_finger2.append(array[1])
    dg_finger3.append(array[2])
    dg_finger4.append(array[3])
    dg_finger5.append(array[4])

# make arrays of 500 samples (500 ms) long with 250 sample (250 ms) sliding window 
# time windows should be: (xLen/fs - winLen + winDisp)/winDisp --> (300000/1000 - 0.5 + 0.25)/0.25 = 1199 
pop1 = dg_finger1; pop2 = dg_finger2; pop3 = dg_finger3; pop4 = dg_finger4; pop5 = dg_finger5
windows1 = []; windows2 = []; windows3 = []; windows4 = []; windows5 = []
while len(pop1) >= 500: 
    temp1 = pop1[0:500]; temp2 = pop2[0:500]; temp3 = pop3[0:500]
    temp4 = pop4[0:500]; temp5 = pop5[0:500]
    windows1.append(temp1); windows2.append(temp2); windows3.append(temp3) 
    windows4.append(temp4); windows5.append(temp5)
    for pop_amount in range(250):
        pop1.pop(0); pop2.pop(0); pop3.pop(0); pop4.pop(0); pop5.pop(0)

# make arrays to track how much each finger changes in each time window 
change1 = []; change2 = []; change3 =[]; change4 = []; change5 =[]

for window in windows1:
    temp_change = 0 
    for i in range(len(window)-1):
        temp_change+= abs(window[i+1] - window[i])
    change1.append(temp_change)

for window in windows2:
    temp_change = 0 
    for i in range(len(window)-1):
        temp_change+= abs(window[i+1] - window[i])
    change2.append(temp_change)

for window in windows3:
    temp_change = 0 
    for i in range(len(window)-1):
        temp_change+= abs(window[i+1] - window[i])
    change3.append(temp_change)

for window in windows4:
    temp_change = 0 
    for i in range(len(window)-1):
        temp_change+= abs(window[i+1] - window[i])
    change4.append(temp_change)
    
for window in windows5:
    temp_change = 0 
    for i in range(len(window)-1):
        temp_change+= abs(window[i+1] - window[i])
    change5.append(temp_change)

# find where there are changes over a time frame for at least one of the fingers 
nonzero_indicies = []
for index in range(len(change1)): 
    if (change1[index] + change2[index] + change3[index] + change4[index] + change5[index]) != 0:
        nonzero_indicies.append(index)

# every time interval has some change in at least one finger --> no need to reduce

# make labels by finding highest change in each interval

list_of_changes = []
for i in range(len(change1)):
    temp = []
    temp.append(change1[i])
    temp.append(change2[i])
    temp.append(change3[i])
    temp.append(change4[i])
    temp.append(change5[i])
    list_of_changes.append(temp)

labels_list = []
for five_set in list_of_changes: 
    labels_list.append(five_set.index(max(five_set)))

# one hot encode these labels 
labels_df = pd.DataFrame(data = labels_list)
labels = np.array(labels_df)
labels = labels.reshape(len(labels_list),)
labels_one_hot = to_categorical(labels)

"""Prepare Inputs"""

# find which channels are the most variant over the entire time scale
full_ecog_p1_list = full_ecog_p1.tolist()
ecog_df = pd.DataFrame(full_ecog_p1_list)

big_list_of_channels = []
for i in range(ecog_df.shape[1]): 
    big_list_of_channels.append(ecog_df[i].tolist())

channel_changes = []
for channel in big_list_of_channels:
    temp = 0
    for i in range(len(channel)-1): 
        temp += abs(channel[i+1] - channel[i])
    channel_changes.append(temp)
# all channels have similar change over entire time scale

# filter the data 
numerator, denominator = scipy.signal.butter(5,(2*200)/1000)
for i in range(62): 
    ecog_df[i] = scipy.signal.lfilter(numerator,denominator,ecog_df[i].tolist())

# # min max scale the data # messed everything up horribly 
# scaler = MinMaxScaler()
# scaled_ecog = scaler.fit_transform(ecog_df)
# ecog_df = pd.DataFrame(scaled_ecog)

# get into arrays consistent with outputs 
for i in range(len(ecog_df)):
    full_ecog_p1_list[i] = ecog_df.loc[i].tolist()

ECOG_windows = np.zeros((len(labels_one_hot),500,62))
count = 0
while len(full_ecog_p1_list) >= 500: 
    ECOG_windows[count,:,:] = full_ecog_p1_list[0:500]
    for pop_amount in range(250):
        full_ecog_p1_list.pop(0)
    count += 1

"""Final Preprocessing and Train Test Split """

X_train = ECOG_windows[0:960]
Y_train = labels_one_hot[0:960]

X_test = ECOG_windows[960:]
Y_test = labels_one_hot[960:]

# shuffle data 
X_train_shuff, Y_train_shuff = shuffle(X_train,Y_train)
X_test_shuff, Y_test_shuff = shuffle(X_test,Y_test)

"""Models and Feature Extraction """

# imports 
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Activation

"""LSTM RNN """

model = Sequential()
model.add(LSTM(50,return_sequences = False, input_shape = (500,62)))
model.add(Dropout(0.2))
model.add(Dense(5,activation = 'softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X_train_shuff, Y_train_shuff, batch_size = 960, epochs = 200, validation_data = (X_test_shuff, Y_test_shuff))

"""Feature Extraction """

def bandpower(x, fmin, fmax):
    f, Pxx = scipy.signal.periodogram(x, fs=1000)
    ind_min = np.argmax(f > fmin) - 1
    ind_max = np.argmax(f > fmax) - 1
    return np.trapz(Pxx[ind_min: ind_max], f[ind_min: ind_max])
def line_length(x):
    return(sum(abs(np.diff(x))))
def area(x):
    return(sum(abs(x)))
def energy(x):
    return(sum(np.square(x)))
def dc_gain(x):
    return(np.mean(x))
def zero_crossings(x):
    return(sum(x > np.mean(x)))
def peak_volt(x):
    return(np.max(x))
def variance(x):
    return(np.std(x)**2)

n_feats = 8 
n_channels = 62
features = np.zeros((len(labels_one_hot),n_feats,62))

for channel in range(n_channels): 
    for epoch in range(len(labels_one_hot)): 
        current_signal = ECOG_windows[epoch,:,channel]
        features[epoch,0,channel] = bandpower(current_signal, 0.15, 200)
        features[epoch,1,channel] = line_length(current_signal) 
        features[epoch,2,channel] = area(current_signal)
        features[epoch,3,channel] = energy(current_signal) 
        features[epoch,4,channel] = dc_gain(current_signal) 
        features[epoch,5,channel] = zero_crossings(current_signal) 
        features[epoch,6,channel] = peak_volt(current_signal) 
        features[epoch,7,channel] = variance(current_signal)

feature_changes = np.zeros((n_feats, n_channels))

#len(features) # 1199
#len(features[0]) # 8 
#len(features[0,0,:]) # 62

for feature in range(len(features[0])): 
    for channel in range(len(features[0,0,:])):
        for epoch in range(len(features)-1): 
            feature_changes[feature,channel] += abs(features[epoch+1,feature,channel] - features[epoch,feature,channel])

highest_variance_channels = []
for i in range(n_feats): 
    highest_variance_channels.append(np.argmax(feature_changes[i,:]))
# indicies of channels with the most change in a singular feature: 54 and 48 
#highest_variance_channels

# set all of channel index 48 and 54 in feature changes to zeros and redo to hopefully get another channel
#feature_changes[:,48] = 0; feature_changes[:,54] = 0
# highest_variance_channels = []
# for i in range(n_feats): 
#     highest_variance_channels.append(np.argmax(feature_changes[i,:]))
# next most variant channel by far is channel index 39
#highest_variance_channels

# then do feature selection on these channels individually to see which of the features are highly correlated 
from sklearn.decomposition import PCA
ch54_feats = features[:,:,54]
ch48_feats = features[:,:,48]
ch39_feats = features[:,:,39]
ch_df = pd.DataFrame(ch54_feats)
ch_df.corr() #pearson correlation matrix 
# find that bandpower and variance are almost perfectly correlated, remove variance

variant_channels = np.zeros((1199,7,3))
variant_channels[:,:,0] = ch54_feats[:,1:8]
variant_channels[:,:,1] = ch48_feats[:,1:8]
variant_channels[:,:,2] = ch39_feats[:,1:8]

X_train_v = variant_channels[0:960]
X_test_v = variant_channels[960:]

Y_train_v = labels_one_hot[0:960]
Y_test_v = labels_one_hot[960:] 

X_train_vshuff, Y_train_vshuff = shuffle(X_train_v,Y_train_v)
X_test_vshuff, Y_test_vshuff = shuffle(X_test_v, Y_test_v)

model = Sequential()
model.add(LSTM(50,return_sequences = False, input_shape = (7,3)))
model.add(Dropout(0.2))
model.add(Dense(5,activation = 'softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X_train_vshuff, Y_train_vshuff, batch_size = 960, epochs = 50, validation_data = (X_test_vshuff, Y_test_vshuff))

# create three different LSTMs and do majority vote based on these channels

# do lstm with only the most variant channels 
new_ECOG = np.zeros((1199,500,3))
new_ECOG[:,:,0] = ECOG_windows[:,:,54]
new_ECOG[:,:,1] = ECOG_windows[:,:,48]
new_ECOG[:,:,2] = ECOG_windows[:,:,39]

X_train_3 = new_ECOG[0:960]
X_test_3 = new_ECOG[960:]

Y_train_3 = labels_one_hot[0:960]
Y_test_3 = labels_one_hot[960:]

X_train_3s, Y_train_3s = shuffle(X_train_3, Y_train_3)
X_test_3s, Y_test_3s = shuffle(X_test_3, Y_test_3)

model = Sequential()
model.add(LSTM(10,return_sequences = False, input_shape = (500,3)))
model.add(Dropout(0.3))
model.add(Dense(5,activation = 'softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X_train_3s, Y_train_3s, batch_size = 960, epochs = 50, validation_data = (X_test_3s, Y_test_3s))

# create one LSTM, CNN, and RF and do ensemble

features.shape

X_train = features[0:960]; X_test = features[960:]
Y_train = labels_one_hot[0:960]; Y_test = labels_one_hot[960:]
X_train_pls, Y_train_pls = shuffle(X_train, Y_train)
X_test_pls, Y_test_pls = shuffle(X_test, Y_test)

model = Sequential()
model.add(LSTM(200,return_sequences = False, input_shape = (8,62)))
model.add(Dropout(0.5))
model.add(Dense(5,activation = 'softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X_train_pls, Y_train_pls, batch_size = 960, epochs = 200, validation_data = (X_test_pls, Y_test_pls))

features.shape

features_2d = np.zeros((1199, 8*62))
for idx in range(1199):
    for chan in range(62):
        for feat in range(8):
            features_2d[idx, chan + feat*n_channels] = features[idx,feat,chan]

# scale
scaler = MinMaxScaler()

features_2d = scaler.fit_transform(pd.DataFrame(features_2d))

X_train = features_2d[0:960]
Y_train = labels_list[0:960]

X_test = features_2d[960:]
Y_test = labels_list[960:]

# shuffle data 
X_train_shuff, Y_train_shuff = shuffle(X_train,Y_train)
X_test_shuff, Y_test_shuff = shuffle(X_test,Y_test)

# train logistic regression classifier
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(random_state=1,verbose=True,max_iter=4000,solver='sag',C=.28).fit(X_train_shuff, Y_train_shuff)

model.score(X_train_shuff, Y_train_shuff)

model.score(X_test_shuff, Y_test_shuff)

model = RandomForestClassifier(max_leaf_nodes = 50, max_depth=40,n_estimators=750)
model.fit(X_train_shuff, Y_train_shuff)
#train_score = model.score(X_train_shuff, Y_train_shuff)
test_score = model.score(X_test_shuff, Y_test_shuff)

test_score

