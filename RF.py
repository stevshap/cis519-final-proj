# -*- coding: utf-8 -*-
"""RandomForest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fLF-UltTb6lDISECxqmX_PMPXqH2B7sl
"""

## IMPORTS 
import numpy as np 
import pandas as pd
import scipy
from scipy import signal
from scipy.io import loadmat
from sklearn import preprocessing 
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import shuffle 
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from keras.utils import to_categorical
import matplotlib.pyplot as plt

# Load in full data set from MATLAB
data = loadmat('cis519project_data.mat')

# Store data into arrays 
full_dg_p1 = data['full_dg_p1']; full_dg_p2 = data['full_dg_p2']; full_dg_p3 = data['full_dg_p3']
full_ecog_p1 = data['full_ecog_p1']; full_ecog_p2 = data['full_ecog_p2']; full_ecog_p3 = data['full_ecog_p3']

"""Prepare Labels """

# make individual finger flexion arrays 
dg_finger1 = []; dg_finger2 = []; dg_finger3 = []; dg_finger4 = []; dg_finger5 = [] 
for array in full_dg_p1: 
    dg_finger1.append(array[0])
    dg_finger2.append(array[1])
    dg_finger3.append(array[2])
    dg_finger4.append(array[3])
    dg_finger5.append(array[4])

# make arrays of 500 samples (500 ms) long with 250 sample (250 ms) sliding window 
# time windows should be: (xLen/fs - winLen + winDisp)/winDisp --> (300000/1000 - 0.5 + 0.25)/0.25 = 1199 
pop1 = dg_finger1; pop2 = dg_finger2; pop3 = dg_finger3; pop4 = dg_finger4; pop5 = dg_finger5
windows1 = []; windows2 = []; windows3 = []; windows4 = []; windows5 = []
while len(pop1) >= 500: 
    temp1 = pop1[0:500]; temp2 = pop2[0:500]; temp3 = pop3[0:500]
    temp4 = pop4[0:500]; temp5 = pop5[0:500]
    windows1.append(temp1); windows2.append(temp2); windows3.append(temp3) 
    windows4.append(temp4); windows5.append(temp5)
    for pop_amount in range(250):
        pop1.pop(0); pop2.pop(0); pop3.pop(0); pop4.pop(0); pop5.pop(0)

# make arrays to track how much each finger changes in each time window 
change1 = []; change2 = []; change3 =[]; change4 = []; change5 =[]

for window in windows1:
    temp_change = 0 
    for i in range(len(window)-1):
        temp_change+= abs(window[i+1] - window[i])
    change1.append(temp_change)

for window in windows2:
    temp_change = 0 
    for i in range(len(window)-1):
        temp_change+= abs(window[i+1] - window[i])
    change2.append(temp_change)

for window in windows3:
    temp_change = 0 
    for i in range(len(window)-1):
        temp_change+= abs(window[i+1] - window[i])
    change3.append(temp_change)

for window in windows4:
    temp_change = 0 
    for i in range(len(window)-1):
        temp_change+= abs(window[i+1] - window[i])
    change4.append(temp_change)
    
for window in windows5:
    temp_change = 0 
    for i in range(len(window)-1):
        temp_change+= abs(window[i+1] - window[i])
    change5.append(temp_change)

# find where there are changes over a time frame for at least one of the fingers 
nonzero_indicies = []
for index in range(len(change1)): 
    if (change1[index] + change2[index] + change3[index] + change4[index] + change5[index]) != 0:
        nonzero_indicies.append(index)

# every time interval has some change in at least one finger --> no need to reduce

# make labels by finding highest change in each interval

list_of_changes = []
for i in range(len(change1)):
    temp = []
    temp.append(change1[i])
    temp.append(change2[i])
    temp.append(change3[i])
    temp.append(change4[i])
    temp.append(change5[i])
    list_of_changes.append(temp)

labels_list = []
for five_set in list_of_changes: 
    labels_list.append(five_set.index(max(five_set)))

# one hot encode these labels 
labels_df = pd.DataFrame(data = labels_list)
labels = np.array(labels_df)
labels = labels.reshape(len(labels_list),)
labels_one_hot = to_categorical(labels)

"""Prepare Inputs"""

# find which channels are the most variant over the entire time scale
full_ecog_p1_list = full_ecog_p1.tolist()
ecog_df = pd.DataFrame(full_ecog_p1_list)

big_list_of_channels = []
for i in range(ecog_df.shape[1]): 
    big_list_of_channels.append(ecog_df[i].tolist())

channel_changes = []
for channel in big_list_of_channels:
    temp = 0
    for i in range(len(channel)-1): 
        temp += abs(channel[i+1] - channel[i])
    channel_changes.append(temp)
# all channels have similar change over entire time scale

# filter the data 
numerator, denominator = scipy.signal.butter(5,(2*200)/1000)
for i in range(62): 
    ecog_df[i] = scipy.signal.lfilter(numerator,denominator,ecog_df[i].tolist())

# get into arrays consistent with outputs 
for i in range(len(ecog_df)):
    full_ecog_p1_list[i] = ecog_df.loc[i].tolist()

np.shape(full_ecog_p1_list)

ECOG_windows = np.zeros((len(labels_one_hot),500,62))
count = 0
while len(full_ecog_p1_list) >= 500: 
    ECOG_windows[count,:,:] = full_ecog_p1_list[0:500]
    for pop_amount in range(250):
        full_ecog_p1_list.pop(0)
    count += 1

np.shape(ECOG_windows)

## CALCULATE FEATURES

def bandpower(x, fmin, fmax):
    f, Pxx = scipy.signal.periodogram(x, fs=1000)
    ind_min = np.argmax(f > fmin) - 1
    ind_max = np.argmax(f > fmax) - 1
    return np.trapz(Pxx[ind_min: ind_max], f[ind_min: ind_max])
def line_length(x):
    return(sum(abs(np.diff(x))))
def area(x):
    return(sum(abs(x)))
def energy(x):
    return(sum(np.square(x)))
def dc_gain(x):
    return(np.mean(x))
def zero_crossings(x):
    return(sum(x > np.mean(x)))
def peak_volt(x):
    return(np.max(x))
def variance(x):
    return(np.std(x)**2)

feat_names = ['BP 8-12', 'BP 18-24', 'BP 75-115', 'BP 125-159', 'BP 160-180', 'Line Length', 'Area', 'Energy', 'DC Gain', 'Zero Crossings', 'Peak Voltage', 'Variance']
n_feats = 12
n_channels = 62
batch_size = 40
batch_ct = len(change1);
features = np.zeros((batch_ct, n_channels, n_feats))
for chan in range(n_channels):
    for idx in range(batch_ct):
        x = ECOG_windows[idx,:,chan]
        features[idx,chan,0] = bandpower(x, 8, 12)
        features[idx,chan,1] = bandpower(x, 18, 24)
        features[idx,chan,2] = bandpower(x, 75, 115)
        features[idx,chan,3] = bandpower(x, 125, 159)
        features[idx,chan,4] = bandpower(x, 160, 180)
        features[idx,chan,5] = line_length(x)
        features[idx,chan,6] = area(x)
        features[idx,chan,7] = energy(x)
        features[idx,chan,8] = dc_gain(x)
        features[idx,chan,9] = zero_crossings(x)
        features[idx,chan,10] = peak_volt(x)
        features[idx,chan,11] = variance(x)
    pd.DataFrame(features[:,chan,:],columns=feat_names).to_csv('Features/feats_500ms_'+str(chan),index=False)

# reduce dimensionality of feature matrix
features_2d = np.zeros((batch_ct, n_channels*n_feats))
for idx in range(batch_ct):
    for chan in range(n_channels):
        for feat in range(n_feats):
            features_2d[idx, chan + feat*n_channels] = features[idx,chan,feat]

# scale
scl = MinMaxScaler()

features_2d = scl.fit_transform(pd.DataFrame(features_2d))

"""Final Preprocessing and Train Test Split """

X_train = features_2d[0:960]
Y_train = labels_list[0:960]

X_test = features_2d[960:]
Y_test = labels_list[960:]

# shuffle data 
X_train_shuff, Y_train_shuff = shuffle(X_train,Y_train)
X_test_shuff, Y_test_shuff = shuffle(X_test,Y_test)

print(np.shape(X_train))
print(np.shape(Y_train))
print(np.shape(X_test))
print(np.shape(Y_test))

"""Train Model

"""

# train model
rf_model = RandomForestClassifier(max_leaf_nodes=70)
rf_model.fit(X_train_shuff, Y_train_shuff)

# get predictions
train_predictions = rf_model.predict(X_train_shuff)
test_predictions = rf_model.predict(X_test_shuff)

# get accuracy
train_score = rf_model.score(X_train_shuff, Y_train_shuff)
print('Train Accuracy:')
print(train_score)
test_score = rf_model.score(X_test_shuff, Y_test_shuff)
print('Test Accuracy:')
print(test_score)

# Optimize Max Leaf Nodes
max_leaves = [2,3,4,5,6,7,8,10,12,15,20,25,30,40,50,75,100,150,200,250]
train_scores = []
test_scores = []
for max_leaf_nodes in max_leaves:
    # shuffle train test set each time???
    model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes)
    model.fit(X_train_shuff, Y_train_shuff)
    train_score = model.score(X_train_shuff, Y_train_shuff)
    test_score = model.score(X_test_shuff, Y_test_shuff)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(str(max_leaf_nodes)+' leaves, train:'+str(train_score)+', test:'+str(test_score))

# plot
plt.plot(train_scores)
plt.plot(test_scores)
plt.xticks(range(len(max_leaves)), max_leaves)
plt.legend(['Train Acc','Test Acc'])
plt.xlabel('Max Leaf nodes')
plt.ylabel('Accuracy')

print(max(test_scores))

## since fingers 3 and 4 are highly correlated, maybe change the class_weight parameter

# Optimize Max Depth
max_depths = [2,3,4,5,6,7,8,10,12,15,20,25,30,40,50,75,100]
train_scores = []
test_scores = []
for max_depth in max_depths:
    # shuffle train test set each time???
    model = RandomForestClassifier(max_depth=max_depth)
    model.fit(X_train_shuff, Y_train_shuff)
    train_score = model.score(X_train_shuff, Y_train_shuff)
    test_score = model.score(X_test_shuff, Y_test_shuff)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(str(max_depth)+' splits, train:'+str(train_score)+', test:'+str(test_score))

# plot
plt.plot(train_scores)
plt.plot(test_scores)
plt.xticks(range(len(max_depths)), max_depths)
plt.legend(['Train Acc','Test Acc'])
plt.xlabel('Max Tree Depth')
plt.ylabel('Accuracy')

# Optimize N Estimators
n_ests = [2,4,6,8,10,50,100,150,200,250,300,400,500,750,1000,2000]
train_scores = []
test_scores = []
for n_est in n_ests:
   # shuffle train test set each time???
    model = RandomForestClassifier(max_leaf_nodes=20,n_estimators=n_est)
    model.fit(X_train_shuff, Y_train_shuff)
    train_score = model.score(X_train_shuff, Y_train_shuff)
    test_score = model.score(X_test_shuff, Y_test_shuff)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(str(max_leaf_nodes)+' leaf nodes, '+str(n_est)+' trees, train:'+str(train_score)+', test:'+str(test_score))

# plot
plt.plot(train_scores)
plt.plot(test_scores)
plt.xticks(range(len(n_ests)), n_ests)
plt.legend(['Train Acc','Test Acc'])
plt.xlabel('Number of Trees')
plt.ylabel('Accuracy')

# Optimize N Estimators and Max Leaf Nodes
n_ests = [10,50,100,150,200,250,300,400,500,750,1000]
train_scores = []
test_scores = []
for max_leaf_nodes in max_leaves:
    for n_est in n_ests:
        # shuffle train test set each time???
        model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes,n_estimators=n_est)
        model.fit(X_train_shuff, Y_train_shuff)
        train_score = model.score(X_train_shuff, Y_train_shuff)
        test_score = model.score(X_test_shuff, Y_test_shuff)
        train_scores.append(train_score)
        test_scores.append(test_score)
        print(str(max_leaf_nodes)+' leaf nodes, '+str(n_est)+' trees, train:'+str(train_score)+', test:'+str(test_score))

print(max(test_scores))

# plot
plt.plot(train_scores)
plt.plot(test_scores)
#plt.xticks(range(len(n_ests)), n_ests)
plt.legend(['Train Acc','Test Acc'])
plt.xlabel('Number of Trees')
plt.ylabel('Accuracy')

# FINAL OPTIMIZED MODEL
final_model = RandomForestClassifier(max_leaf_nodes=50,n_estimators=250)
final_model.fit(X_train_shuff, Y_train_shuff)
# get accuracy
train_score = final_model.score(X_train_shuff, Y_train_shuff)
print('Train Accuracy:')
print(train_score)
test_score = final_model.score(X_test_shuff, Y_test_shuff)
print('Test Accuracy:')
print(test_score)



